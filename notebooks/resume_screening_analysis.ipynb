{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c99e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume Screening Automation - Complete Analysis\n",
    "# This notebook performs end-to-end resume screening analysis\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 1: Import Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data_preprocessing import ResumePreprocessor, load_and_preprocess_data\n",
    "from feature_extraction import FeatureExtractor, ResumeMatcher\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 2: Load and Explore Data\n",
    "# ============================================================================\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/raw/Resume.csv')\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nðŸ“Š Basic Statistics:\")\n",
    "print(f\"Total Resumes: {len(df)}\")\n",
    "print(f\"Unique Categories: {df['Category'].nunique()}\")\n",
    "print(f\"Missing Values:\\n{df.isnull().sum()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 3: Category Distribution Analysis\n",
    "# ============================================================================\n",
    "\n",
    "# Analyze category distribution\n",
    "category_counts = df['Category'].value_counts()\n",
    "\n",
    "print(\"\\nðŸ“ˆ Category Distribution:\")\n",
    "print(category_counts)\n",
    "\n",
    "# Visualize category distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot\n",
    "category_counts.plot(kind='bar', ax=axes[0], color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Resume Count by Category', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Category', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart (top 10 categories)\n",
    "top_10_categories = category_counts.head(10)\n",
    "axes[1].pie(top_10_categories, labels=top_10_categories.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Top 10 Categories Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 4: Preprocess Resume Data\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = ResumePreprocessor()\n",
    "\n",
    "# Process the entire dataset\n",
    "print(\"ðŸ”„ Preprocessing resumes...\")\n",
    "df_processed = preprocessor.process_dataframe(df, text_column='Resume_str')\n",
    "\n",
    "print(\"\\nâœ… Preprocessing completed!\")\n",
    "print(f\"New columns added: {[col for col in df_processed.columns if col not in df.columns]}\")\n",
    "\n",
    "# Display sample processed data\n",
    "print(\"\\nðŸ“ Sample Processed Resume:\")\n",
    "print(f\"Original (first 200 chars):\\n{df_processed.iloc[0]['Resume_str'][:200]}\")\n",
    "print(f\"\\nCleaned (first 200 chars):\\n{df_processed.iloc[0]['cleaned_resume'][:200]}\")\n",
    "print(f\"\\nExtracted Skills: {df_processed.iloc[0]['skills'][:10]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 5: Resume Length Analysis\n",
    "# ============================================================================\n",
    "\n",
    "# Analyze resume lengths\n",
    "print(\"\\nðŸ“ Resume Length Statistics:\")\n",
    "print(df_processed[['resume_length', 'cleaned_length', 'num_skills']].describe())\n",
    "\n",
    "# Visualize resume lengths\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Original resume length distribution\n",
    "axes[0, 0].hist(df_processed['resume_length'], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Distribution of Original Resume Length', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Number of Words')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(df_processed['resume_length'].mean(), color='red', linestyle='--', \n",
    "                    label=f\"Mean: {df_processed['resume_length'].mean():.0f}\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Cleaned resume length distribution\n",
    "axes[0, 1].hist(df_processed['cleaned_length'], bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('Distribution of Cleaned Resume Length', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Number of Words')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].axvline(df_processed['cleaned_length'].mean(), color='red', linestyle='--', \n",
    "                    label=f\"Mean: {df_processed['cleaned_length'].mean():.0f}\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Number of skills distribution\n",
    "axes[1, 0].hist(df_processed['num_skills'], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_title('Distribution of Identified Skills', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Number of Skills')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].axvline(df_processed['num_skills'].mean(), color='red', linestyle='--', \n",
    "                    label=f\"Mean: {df_processed['num_skills'].mean():.1f}\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Box plot: Skills by category (top 10 categories)\n",
    "top_categories = df_processed['Category'].value_counts().head(10).index\n",
    "df_top = df_processed[df_processed['Category'].isin(top_categories)]\n",
    "df_top.boxplot(column='num_skills', by='Category', ax=axes[1, 1], patch_artist=True)\n",
    "axes[1, 1].set_title('Skills Distribution by Category (Top 10)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Category')\n",
    "axes[1, 1].set_ylabel('Number of Skills')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 6: Skills Analysis\n",
    "# ============================================================================\n",
    "\n",
    "# Extract all skills and count frequencies\n",
    "from collections import Counter\n",
    "\n",
    "all_skills = []\n",
    "for skills_list in df_processed['skills']:\n",
    "    all_skills.extend(skills_list)\n",
    "\n",
    "skill_counts = Counter(all_skills)\n",
    "top_skills = skill_counts.most_common(20)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Top 20 Skills Across All Resumes:\")\n",
    "for skill, count in top_skills:\n",
    "    print(f\"{skill}: {count}\")\n",
    "\n",
    "# Visualize top skills\n",
    "skills_df = pd.DataFrame(top_skills, columns=['Skill', 'Count'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(skills_df['Skill'], skills_df['Count'], color='teal', edgecolor='black')\n",
    "plt.xlabel('Frequency', fontsize=12)\n",
    "plt.ylabel('Skill', fontsize=12)\n",
    "plt.title('Top 20 Most Common Skills', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 7: Word Cloud Visualization\n",
    "# ============================================================================\n",
    "\n",
    "# Create word cloud from all cleaned resumes\n",
    "all_text = ' '.join(df_processed['cleaned_resume'].astype(str))\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800, \n",
    "                      background_color='white', \n",
    "                      colormap='viridis',\n",
    "                      max_words=100).generate(all_text)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of All Resumes', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Word clouds by category (top 4 categories)\n",
    "top_4_categories = df_processed['Category'].value_counts().head(4).index\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, category in enumerate(top_4_categories):\n",
    "    category_text = ' '.join(df_processed[df_processed['Category'] == category]['cleaned_resume'].astype(str))\n",
    "    \n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color='white', \n",
    "                   colormap='plasma',\n",
    "                   max_words=50).generate(category_text)\n",
    "    \n",
    "    axes[idx].imshow(wc, interpolation='bilinear')\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f'Word Cloud: {category}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 8: TF-IDF Feature Extraction\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize feature extractor\n",
    "extractor = FeatureExtractor(max_features=1000, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform\n",
    "print(\"ðŸ”§ Extracting TF-IDF features...\")\n",
    "tfidf_matrix, encoded_labels = extractor.fit_transform(\n",
    "    df_processed['cleaned_resume'].tolist(),\n",
    "    df_processed['Category'].tolist()\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… TF-IDF extraction completed!\")\n",
    "print(f\"Feature matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Number of unique labels: {len(set(encoded_labels))}\")\n",
    "\n",
    "# Get feature names\n",
    "feature_names = extractor.get_feature_names()\n",
    "print(f\"\\nSample features: {feature_names[:20]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 9: Top TF-IDF Features by Category\n",
    "# ============================================================================\n",
    "\n",
    "# Function to get top features for each category\n",
    "def get_category_top_features(df, extractor, category, top_n=15):\n",
    "    \"\"\"Get top TF-IDF features for a specific category\"\"\"\n",
    "    category_resumes = df[df['Category'] == category]['cleaned_resume'].tolist()\n",
    "    combined_text = ' '.join(category_resumes)\n",
    "    return extractor.get_top_features(combined_text, top_n=top_n)\n",
    "\n",
    "# Analyze top 3 categories\n",
    "top_3_categories = df_processed['Category'].value_counts().head(3).index\n",
    "\n",
    "print(\"\\nðŸ” Top TF-IDF Features by Category:\\n\")\n",
    "for category in top_3_categories:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    top_features = get_category_top_features(df_processed, extractor, category, top_n=15)\n",
    "    \n",
    "    for feature, score in top_features:\n",
    "        print(f\"  {feature:30s} : {score:.4f}\")\n",
    "\n",
    "# Visualize top features for each category\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, category in enumerate(top_3_categories):\n",
    "    top_features = get_category_top_features(df_processed, extractor, category, top_n=10)\n",
    "    features, scores = zip(*top_features)\n",
    "    \n",
    "    axes[idx].barh(features, scores, color='purple', alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_xlabel('TF-IDF Score', fontsize=10)\n",
    "    axes[idx].set_title(f'{category}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].invert_yaxis()\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 10: Resume-Job Matching Example\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize matcher\n",
    "matcher = ResumeMatcher(extractor)\n",
    "\n",
    "# Sample job descriptions for different categories\n",
    "job_descriptions = {\n",
    "    'Data Science': \"\"\"\n",
    "        Looking for a Data Scientist with strong Python programming skills.\n",
    "        Must have experience with machine learning, deep learning, TensorFlow, and scikit-learn.\n",
    "        Knowledge of SQL, pandas, numpy, and data visualization is required.\n",
    "        Experience with AWS or cloud platforms is a plus.\n",
    "    \"\"\",\n",
    "    \n",
    "    'Web Development': \"\"\"\n",
    "        Seeking a Full Stack Web Developer proficient in JavaScript, React, and Node.js.\n",
    "        Experience with HTML, CSS, and responsive design required.\n",
    "        Knowledge of databases (SQL, MongoDB) and RESTful APIs essential.\n",
    "        Familiarity with Git, Agile methodologies, and CI/CD is preferred.\n",
    "    \"\"\",\n",
    "    \n",
    "    'Java Developer': \"\"\"\n",
    "        Looking for Java Developer with expertise in Spring Boot and microservices.\n",
    "        Must have experience with Docker, Kubernetes, and cloud deployment.\n",
    "        Strong knowledge of SQL, REST APIs, and software design patterns required.\n",
    "        Experience with Jenkins, Git, and Agile development is a plus.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Test matching for Data Science category\n",
    "print(\"\\nðŸŽ¯ Resume Matching Example: Data Science Position\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"Job Description:\")\n",
    "print(job_descriptions['Data Science'])\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get top 5 matches\n",
    "top_matches = matcher.rank_candidates(\n",
    "    df_processed[df_processed['Category'] == 'Data Science'],\n",
    "    job_descriptions['Data Science'],\n",
    "    top_n=5\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ† Top 5 Candidate Matches:\\n\")\n",
    "for idx, row in top_matches.iterrows():\n",
    "    print(f\"\\nCandidate {idx}:\")\n",
    "    print(f\"  Match Score: {row['match_percentage']:.2f}%\")\n",
    "    print(f\"  Skills: {row['skills'][:10]}\")\n",
    "    print(f\"  Resume Preview: {row['Resume_str'][:150]}...\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 11: Detailed Match Analysis\n",
    "# ============================================================================\n",
    "\n",
    "# Get detailed analysis for top candidate\n",
    "if len(top_matches) > 0:\n",
    "    top_candidate = top_matches.iloc[0]\n",
    "    \n",
    "    analysis = matcher.get_match_analysis(\n",
    "        top_candidate['cleaned_resume'],\n",
    "        job_descriptions['Data Science']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nðŸ”¬ Detailed Match Analysis for Top Candidate:\\n\")\n",
    "    print(f\"Overall Match: {analysis['match_percentage']:.2f}%\")\n",
    "    print(f\"\\nâœ… Matching Keywords ({len(analysis['matching_keywords'])}):\")\n",
    "    print(\", \".join(analysis['matching_keywords'][:15]))\n",
    "    \n",
    "    print(f\"\\nâŒ Missing Keywords ({len(analysis['missing_keywords'])}):\")\n",
    "    print(\", \".join(analysis['missing_keywords'][:15]))\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Candidate's Top Keywords:\")\n",
    "    for keyword, score in analysis['resume_top_keywords'][:10]:\n",
    "        print(f\"  {keyword:25s} : {score:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Job's Top Keywords:\")\n",
    "    for keyword, score in analysis['job_top_keywords'][:10]:\n",
    "        print(f\"  {keyword:25s} : {score:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 12: Cross-Category Matching Analysis\n",
    "# ============================================================================\n",
    "\n",
    "# Test how resumes from different categories match with a specific job\n",
    "print(\"\\nðŸ“Š Cross-Category Matching Analysis\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "categories_to_test = ['Data Science', 'Java Developer', 'Web Designing']\n",
    "test_job = job_descriptions['Data Science']\n",
    "\n",
    "results = []\n",
    "\n",
    "for category in categories_to_test:\n",
    "    category_resumes = df_processed[df_processed['Category'] == category].head(50)\n",
    "    \n",
    "    if len(category_resumes) > 0:\n",
    "        similarities = extractor.batch_similarity(\n",
    "            category_resumes['cleaned_resume'].tolist(),\n",
    "            test_job\n",
    "        )\n",
    "        \n",
    "        avg_similarity = similarities.mean()\n",
    "        max_similarity = similarities.max()\n",
    "        \n",
    "        results.append({\n",
    "            'Category': category,\n",
    "            'Avg Match': avg_similarity * 100,\n",
    "            'Max Match': max_similarity * 100\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize cross-category matching\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, results_df['Avg Match'], width, label='Average Match', color='steelblue', edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, results_df['Max Match'], width, label='Max Match', color='coral', edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Category', fontsize=12)\n",
    "ax.set_ylabel('Match Percentage (%)', fontsize=12)\n",
    "ax.set_title('Cross-Category Matching with Data Science Job', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['Category'])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 13: Save Processed Data and Models\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save processed dataframe\n",
    "print(\"\\nðŸ’¾ Saving processed data and models...\")\n",
    "\n",
    "df_processed.to_csv('../data/processed/processed_resumes.csv', index=False)\n",
    "print(\"âœ… Processed data saved to: ../data/processed/processed_resumes.csv\")\n",
    "\n",
    "# Save vectorizer and label encoder\n",
    "extractor.save_vectorizer('../models/tfidf_vectorizer.pkl')\n",
    "extractor.save_label_encoder('../models/label_encoder.pkl')\n",
    "\n",
    "print(\"\\nðŸŽ‰ Analysis complete! All models and data saved successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 14: Summary Statistics\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ˆ FINAL SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = {\n",
    "    'Total Resumes Processed': len(df_processed),\n",
    "    'Total Categories': df_processed['Category'].nunique(),\n",
    "    'Average Resume Length (words)': df_processed['resume_length'].mean(),\n",
    "    'Average Skills per Resume': df_processed['num_skills'].mean(),\n",
    "    'TF-IDF Features Extracted': tfidf_matrix.shape[1],\n",
    "    'Most Common Category': df_processed['Category'].value_counts().index[0],\n",
    "    'Top Skill Overall': max(skill_counts, key=skill_counts.get)\n",
    "}\n",
    "\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key:.<50} {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{key:.<50} {value}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a4358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 15: Train Classification Models\n",
    "# ============================================================================\n",
    "\n",
    "from src.model_training import ResumeClassifier, train_and_evaluate_models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING CLASSIFICATION MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_matrix, \n",
    "    encoded_labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=encoded_labels\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Train and evaluate multiple models\n",
    "results = train_and_evaluate_models(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    extractor.get_feature_names(),\n",
    "    extractor.label_encoder\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 16: Model Comparison Visualization\n",
    "# ============================================================================\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Extract metrics from results\n",
    "models = list(results.keys())\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "\n",
    "comparison_data = {\n",
    "    metric: [results[model]['metrics'][metric] for model in models]\n",
    "    for metric in metrics\n",
    "}\n",
    "\n",
    "# Create grouped bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "for metric in metrics:\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=metric.capitalize(),\n",
    "        x=models,\n",
    "        y=comparison_data[metric],\n",
    "        text=[f\"{val:.3f}\" for val in comparison_data[metric]],\n",
    "        textposition='auto'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Display numerical comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    model: {\n",
    "        'Accuracy': results[model]['metrics']['accuracy'],\n",
    "        'Precision': results[model]['metrics']['precision'],\n",
    "        'Recall': results[model]['metrics']['recall'],\n",
    "        'F1-Score': results[model]['metrics']['f1_score']\n",
    "    }\n",
    "    for model in results\n",
    "}).T\n",
    "\n",
    "print(\"\\nðŸ“Š Numerical Comparison:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_model = comparison_df['Accuracy'].idxmax()\n",
    "print(f\"\\nðŸ† Best Model: {best_model.upper()}\")\n",
    "print(f\"   Accuracy: {comparison_df.loc[best_model, 'Accuracy']:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 17: Confusion Matrix Visualization\n",
    "# ============================================================================\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Get best model\n",
    "best_classifier = results[best_model]['classifier']\n",
    "best_metrics = results[best_model]['metrics']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "cm = best_metrics['confusion_matrix']\n",
    "categories = extractor.label_encoder.classes_\n",
    "\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=categories,\n",
    "    yticklabels=categories,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "\n",
    "plt.title(f'Confusion Matrix - {best_model.upper()}', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 18: Feature Importance (Random Forest)\n",
    "# ============================================================================\n",
    "\n",
    "if best_model == 'random_forest':\n",
    "    print(\"ðŸŒ² Random Forest Feature Importance\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    importance_df = best_classifier.get_feature_importance(\n",
    "        extractor.get_feature_names(),\n",
    "        top_n=20\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTop 20 Most Important Features:\")\n",
    "    print(importance_df.to_string(index=False))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    fig = px.bar(\n",
    "        importance_df,\n",
    "        x='importance',\n",
    "        y='feature',\n",
    "        orientation='h',\n",
    "        title='Top 20 Feature Importance (Random Forest)',\n",
    "        labels={'importance': 'Importance Score', 'feature': 'Feature'},\n",
    "        color='importance',\n",
    "        color_continuous_scale='Viridis'\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(yaxis={'categoryorder': 'total ascending'}, height=600)\n",
    "    fig.show()\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 19: Advanced Resume Matching\n",
    "# ============================================================================\n",
    "\n",
    "from src.resume_matcher import AdvancedResumeMatcher, JobDescriptionParser\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ADVANCED RESUME MATCHING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create advanced matcher\n",
    "advanced_matcher = AdvancedResumeMatcher(extractor, best_classifier)\n",
    "\n",
    "# Sample job description with requirements\n",
    "job_description = \"\"\"\n",
    "Senior Data Scientist Position\n",
    "\n",
    "We are seeking an experienced Data Scientist with 3-5 years of experience\n",
    "to join our growing team. \n",
    "\n",
    "Required Skills:\n",
    "- Python programming\n",
    "- Machine Learning and Deep Learning\n",
    "- TensorFlow, PyTorch, or scikit-learn\n",
    "- SQL and database management\n",
    "- Data visualization (Tableau, Power BI)\n",
    "- AWS or Azure cloud platforms\n",
    "\n",
    "Nice to Have:\n",
    "- PhD or Master's degree in Computer Science, Statistics, or related field\n",
    "- Experience with big data technologies (Spark, Hadoop)\n",
    "- Natural Language Processing (NLP) experience\n",
    "- Strong communication and teamwork skills\n",
    "\n",
    "Responsibilities include building ML models, analyzing large datasets,\n",
    "and collaborating with cross-functional teams.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nðŸ“‹ Job Description:\")\n",
    "print(job_description[:200] + \"...\")\n",
    "\n",
    "# Extract requirements from job description\n",
    "required_skills = JobDescriptionParser.extract_required_skills(job_description)\n",
    "experience_req = JobDescriptionParser.extract_experience_requirements(job_description)\n",
    "education_req = JobDescriptionParser.extract_education_requirements(job_description)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Extracted Requirements:\")\n",
    "print(f\"  Required Skills: {required_skills[:10]}\")\n",
    "print(f\"  Experience: {experience_req['min_years']}-{experience_req['max_years']} years\")\n",
    "print(f\"  Education: {education_req}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 20: Hybrid Ranking\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ” Performing Hybrid Ranking...\")\n",
    "\n",
    "# Filter to Data Science category\n",
    "data_science_resumes = df_processed[df_processed['Category'] == 'Data Science']\n",
    "\n",
    "# Perform hybrid ranking (TF-IDF + Skills)\n",
    "top_candidates = advanced_matcher.hybrid_ranking(\n",
    "    data_science_resumes,\n",
    "    job_description,\n",
    "    required_skills=required_skills,\n",
    "    tfidf_weight=0.6,\n",
    "    skill_weight=0.4,\n",
    "    top_n=10\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Found {len(top_candidates)} top candidates\")\n",
    "\n",
    "# Display top 5\n",
    "print(\"\\nðŸ† Top 5 Candidates:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, (_, candidate) in enumerate(top_candidates.head(5).iterrows(), 1):\n",
    "    print(f\"\\nCandidate #{idx}\")\n",
    "    print(f\"  Overall Match: {candidate['match_percentage']:.2f}%\")\n",
    "    print(f\"  TF-IDF Score: {candidate['tfidf_score']:.3f}\")\n",
    "    print(f\"  Skill Score: {candidate['skill_score']:.3f}\")\n",
    "    print(f\"  Total Skills: {candidate['num_skills']}\")\n",
    "    print(f\"  Top Skills: {', '.join(candidate['skills'][:5])}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 21: Detailed Match Report\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“„ Detailed Match Report for Top Candidate\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get top candidate\n",
    "top_candidate = top_candidates.iloc[0]\n",
    "\n",
    "# Generate detailed report\n",
    "detailed_report = advanced_matcher.get_detailed_match_report(\n",
    "    top_candidate['cleaned_resume'],\n",
    "    job_description,\n",
    "    required_skills=required_skills\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Match Score: {detailed_report['match_percentage']:.2f}%\")\n",
    "print(f\"ðŸ“Š Recommendation: {detailed_report['recommendation']}\")\n",
    "\n",
    "print(f\"\\nâœ… Matching Keywords ({len(detailed_report['matching_keywords'])}):\")\n",
    "print(\", \".join(detailed_report['matching_keywords'][:15]))\n",
    "\n",
    "print(f\"\\nâŒ Missing Keywords ({len(detailed_report['missing_keywords'])}):\")\n",
    "print(\", \".join(detailed_report['missing_keywords'][:15]))\n",
    "\n",
    "if detailed_report['skill_analysis']:\n",
    "    print(f\"\\nðŸŽ“ Skill Analysis:\")\n",
    "    print(f\"  Matched Skills ({len(detailed_report['skill_analysis']['matched_skills'])}):\")\n",
    "    print(f\"    {', '.join(detailed_report['skill_analysis']['matched_skills'][:10])}\")\n",
    "    print(f\"  Missing Skills ({len(detailed_report['skill_analysis']['missing_skills'])}):\")\n",
    "    print(f\"    {', '.join(detailed_report['skill_analysis']['missing_skills'][:10])}\")\n",
    "    print(f\"  Skill Match: {detailed_report['skill_analysis']['skill_match_percentage']:.1f}%\")\n",
    "\n",
    "print(f\"\\nðŸ“Œ Top Resume Keywords:\")\n",
    "for keyword, score in detailed_report['resume_top_keywords'][:8]:\n",
    "    print(f\"  {keyword:25s} : {score:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 22: Batch Candidate Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“Š Batch Candidate Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze all candidates\n",
    "batch_results = advanced_matcher.batch_match_analysis(\n",
    "    data_science_resumes,\n",
    "    job_description,\n",
    "    required_skills=required_skills,\n",
    "    threshold=0.3\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "summary = batch_results['summary']\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Summary:\")\n",
    "print(f\"  Total Candidates Analyzed: {summary['total_candidates']}\")\n",
    "print(f\"  Strong Match (â‰¥70%): {summary['strong_match_count']}\")\n",
    "print(f\"  Good Match (50-70%): {summary['good_match_count']}\")\n",
    "print(f\"  Moderate Match (30-50%): {summary['moderate_match_count']}\")\n",
    "print(f\"  Weak Match (<30%): {summary['weak_match_count']}\")\n",
    "\n",
    "# Visualize distribution\n",
    "categories_count = [\n",
    "    summary['strong_match_count'],\n",
    "    summary['good_match_count'],\n",
    "    summary['moderate_match_count'],\n",
    "    summary['weak_match_count']\n",
    "]\n",
    "\n",
    "category_labels = ['Strong Match', 'Good Match', 'Moderate Match', 'Weak Match']\n",
    "colors = ['green', 'lightgreen', 'orange', 'red']\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(\n",
    "    labels=category_labels,\n",
    "    values=categories_count,\n",
    "    hole=0.4,\n",
    "    marker_colors=colors\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Candidate Distribution by Match Quality',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Display strong matches\n",
    "if len(batch_results['strong_match']) > 0:\n",
    "    print(f\"\\nðŸŒŸ Strong Match Candidates:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for idx, (_, candidate) in enumerate(batch_results['strong_match'].head(5).iterrows(), 1):\n",
    "        print(f\"\\n{idx}. Match: {candidate['match_percentage']:.2f}%\")\n",
    "        print(f\"   Skills: {', '.join(candidate['skills'][:5])}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 23: Cross-Category Performance Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ” Cross-Category Performance Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test the same job on different categories\n",
    "test_categories = ['Data Science', 'Information-Technology', 'Engineering']\n",
    "category_performance = []\n",
    "\n",
    "for category in test_categories:\n",
    "    category_resumes = df_processed[df_processed['Category'] == category].head(100)\n",
    "    \n",
    "    if len(category_resumes) > 0:\n",
    "        # Calculate similarities\n",
    "        similarities = extractor.batch_similarity(\n",
    "            category_resumes['cleaned_resume'].tolist(),\n",
    "            job_description\n",
    "        )\n",
    "        \n",
    "        category_performance.append({\n",
    "            'Category': category,\n",
    "            'Avg Match': similarities.mean() * 100,\n",
    "            'Max Match': similarities.max() * 100,\n",
    "            'Min Match': similarities.min() * 100,\n",
    "            'Std Dev': similarities.std() * 100,\n",
    "            'Median Match': np.median(similarities) * 100\n",
    "        })\n",
    "\n",
    "perf_df = pd.DataFrame(category_performance)\n",
    "print(\"\\nðŸ“Š Performance Metrics:\")\n",
    "print(perf_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Average Match',\n",
    "    x=perf_df['Category'],\n",
    "    y=perf_df['Avg Match'],\n",
    "    marker_color='steelblue'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Max Match',\n",
    "    x=perf_df['Category'],\n",
    "    y=perf_df['Max Match'],\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Cross-Category Match Performance',\n",
    "    xaxis_title='Category',\n",
    "    yaxis_title='Match Percentage (%)',\n",
    "    barmode='group',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 24: Save All Trained Models\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"\\nðŸ’¾ Saving All Trained Models\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save all classification models\n",
    "for model_name, model_data in results.items():\n",
    "    model_path = f'models/{model_name}_model.pkl'\n",
    "    model_data['classifier'].save_model(model_path)\n",
    "\n",
    "# Save vectorizer and label encoder (if not already saved)\n",
    "extractor.save_vectorizer('models/tfidf_vectorizer.pkl')\n",
    "extractor.save_label_encoder('models/label_encoder.pkl')\n",
    "\n",
    "print(\"\\nâœ… All models saved successfully!\")\n",
    "print(\"\\nSaved models:\")\n",
    "print(\"  - models/tfidf_vectorizer.pkl\")\n",
    "print(\"  - models/label_encoder.pkl\")\n",
    "for model_name in results.keys():\n",
    "    print(f\"  - models/{model_name}_model.pkl\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 25: Final Summary and Recommendations\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ‰ RESUME SCREENING AUTOMATION - COMPLETE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š Dataset Statistics:\")\n",
    "print(f\"  Total Resumes: {len(df_processed):,}\")\n",
    "print(f\"  Categories: {df_processed['Category'].nunique()}\")\n",
    "print(f\"  Average Skills per Resume: {df_processed['num_skills'].mean():.1f}\")\n",
    "print(f\"  Average Resume Length: {df_processed['resume_length'].mean():.0f} words\")\n",
    "\n",
    "print(\"\\nðŸ¤– Model Performance:\")\n",
    "print(f\"  Best Model: {best_model.upper()}\")\n",
    "print(f\"  Accuracy: {comparison_df.loc[best_model, 'Accuracy']:.4f}\")\n",
    "print(f\"  F1-Score: {comparison_df.loc[best_model, 'F1-Score']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Resume Matching Results:\")\n",
    "print(f\"  Top Match Score: {top_candidates.iloc[0]['match_percentage']:.2f}%\")\n",
    "print(f\"  Average Top 10 Match: {top_candidates['match_percentage'].mean():.2f}%\")\n",
    "print(f\"  Strong Matches Found: {summary['strong_match_count']}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Recommendations:\")\n",
    "print(\"  1. Use the best model for category prediction\")\n",
    "print(\"  2. Combine TF-IDF and skill matching for best results\")\n",
    "print(\"  3. Adjust matching thresholds based on position criticality\")\n",
    "print(\"  4. Regularly retrain models with new resume data\")\n",
    "print(\"  5. Use the Streamlit dashboard for interactive matching\")\n",
    "\n",
    "print(\"\\nðŸš€ Next Steps:\")\n",
    "print(\"  1. Deploy models to production\")\n",
    "print(\"  2. Integrate with ATS systems\")\n",
    "print(\"  3. Set up automated email notifications\")\n",
    "print(\"  4. Create API endpoints for resume submission\")\n",
    "print(\"  5. Implement continuous model retraining\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Analysis Complete! All models and data saved.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
